{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Entrega 2 Limpeza y preparación del dataset"
      ],
      "metadata": {
        "id": "e6-cz5aqqqJ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\"
      ],
      "metadata": {
        "id": "HJuKQu3kMA6o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wSCtkc6bq39z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#descomprimir archivos\n",
        "!ls\n",
        "!unzip udea-ai4eng-20242.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anxRcTBaGAkM",
        "outputId": "51624ef9-173e-47ec-8c0c-531036122a17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data  udea-ai4eng-20242.zip\n",
            "Archive:  udea-ai4eng-20242.zip\n",
            "  inflating: submission_example.csv  \n",
            "  inflating: test.csv                \n",
            "  inflating: train.csv               \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entrega final modleo 1"
      ],
      "metadata": {
        "id": "N_MK7EKLRQvn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# solucion 2\n"
      ],
      "metadata": {
        "id": "yeNkMQqTdrmi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xgboost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7gKfId7uTbz",
        "outputId": "cc1454c2-e9db-49e7-9461-69d5404a1ee4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (2.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.26.4)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.10/dist-packages (from xgboost) (2.23.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.13.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "class SaberProXGBoostPipeline:\n",
        "    def __init__(self):\n",
        "        self.pipeline = None\n",
        "        self.label_encoder = LabelEncoder()\n",
        "\n",
        "    def create_preprocessing_pipeline(self, df):\n",
        "        \"\"\"\n",
        "        Crea un pipeline de preprocesamiento optimizado para XGBoost\n",
        "        \"\"\"\n",
        "        # Mapeos para variables categóricas\n",
        "        self.estrato_map = {\n",
        "            \"Estrato 1\": 1, \"Estrato 2\": 2, \"Estrato 3\": 3,\n",
        "            \"Estrato 4\": 4, \"Estrato 5\": 5, \"Estrato 6\": 6\n",
        "        }\n",
        "\n",
        "        self.horas_trabajo_map = {\n",
        "            '0': 0, 'Menos de 10 horas': 5,\n",
        "            'Entre 11 y 20 horas': 15, 'Entre 21 y 30 horas': 25,\n",
        "            'Más de 30 horas': 30\n",
        "        }\n",
        "\n",
        "        self.valor_matricula_map = {\n",
        "            'Entre 1 millón y menos de 2.5 millones': 1.75,\n",
        "            'Entre 2.5 millones y menos de 4 millones': 3.25,\n",
        "            'Menos de 500 mil': 0.25,\n",
        "            'Entre 500 mil y menos de 1 millón': 0.75,\n",
        "            'Entre 4 millones y menos de 5.5 millones': 4.75,\n",
        "            'Más de 7 millones': 7.75,\n",
        "            'Entre 5.5 millones y menos de 7 millones': 6.25,\n",
        "            'No pagó matrícula': 0\n",
        "        }\n",
        "\n",
        "        # Características numéricas y categóricas\n",
        "        numeric_features = ['FAMI_ESTRATOVIVIENDA', 'ESTU_HORASSEMANATRABAJA',\n",
        "                          'ESTU_VALORMATRICULAUNIVERSIDAD', 'FAMI_TIENEINTERNET',\n",
        "                          'ESTU_PAGOMATRICULAPROPIO']\n",
        "\n",
        "        categorical_features = ['ESTU_PRGM_DEPARTAMENTO']\n",
        "\n",
        "        # Transformadores\n",
        "        numeric_transformer = Pipeline(steps=[\n",
        "            ('scaler', StandardScaler())\n",
        "        ])\n",
        "\n",
        "        categorical_transformer = Pipeline(steps=[\n",
        "            ('onehot', OneHotEncoder(drop='first', sparse_output=False))\n",
        "        ])\n",
        "\n",
        "        # Preprocessor\n",
        "        preprocessor = ColumnTransformer(\n",
        "            transformers=[\n",
        "                ('num', numeric_transformer, numeric_features),\n",
        "                ('cat', categorical_transformer, categorical_features)\n",
        "            ])\n",
        "\n",
        "        # XGBoost con configuración inicial optimizada\n",
        "        xgb_classifier = XGBClassifier(\n",
        "            n_estimators=500,\n",
        "            learning_rate=0.1,\n",
        "            max_depth=6,\n",
        "            min_child_weight=1,\n",
        "            gamma=0,\n",
        "            subsample=0.8,\n",
        "            colsample_bytree=0.8,\n",
        "            objective='multi:softprob',\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "        # Pipeline completo\n",
        "        self.pipeline = Pipeline([\n",
        "            ('preprocessor', preprocessor),\n",
        "            ('classifier', xgb_classifier)\n",
        "        ])\n",
        "\n",
        "        return self.pipeline\n",
        "\n",
        "    def prepare_data(self, df):\n",
        "        \"\"\"\n",
        "        Prepara los datos aplicando las transformaciones necesarias\n",
        "        \"\"\"\n",
        "        df_processed = df.copy()\n",
        "\n",
        "        # Eliminar columnas no necesarias\n",
        "        columns_to_drop = ['ID', 'PERIODO', 'FAMI_EDUCACIONPADRE',\n",
        "                          'FAMI_EDUCACIONMADRE', 'ESTU_PRGM_ACADEMICO']\n",
        "        df_processed = df_processed.drop(columns_to_drop, axis=1, errors='ignore')\n",
        "\n",
        "        # Aplicar mapeos con manejo de valores faltantes\n",
        "        df_processed['FAMI_ESTRATOVIVIENDA'] = df_processed['FAMI_ESTRATOVIVIENDA'].fillna('Estrato 3').map(self.estrato_map)\n",
        "        df_processed['ESTU_HORASSEMANATRABAJA'] = df_processed['ESTU_HORASSEMANATRABAJA'].fillna('Más de 30 horas').map(self.horas_trabajo_map)\n",
        "        df_processed['ESTU_VALORMATRICULAUNIVERSIDAD'] = df_processed['ESTU_VALORMATRICULAUNIVERSIDAD'].fillna('Entre 1 millón y menos de 2.5 millones').map(self.valor_matricula_map)\n",
        "        df_processed['FAMI_TIENEINTERNET'] = df_processed['FAMI_TIENEINTERNET'].fillna('Si').map({'Si': 1, 'No': 0})\n",
        "        df_processed['ESTU_PAGOMATRICULAPROPIO'] = df_processed['ESTU_PAGOMATRICULAPROPIO'].fillna('No').map({'Si': 1, 'No': 0})\n",
        "\n",
        "        return df_processed\n",
        "\n",
        "    def train_model(self, X_train, y_train):\n",
        "        \"\"\"\n",
        "        Entrena el modelo usando GridSearchCV con parámetros optimizados para XGBoost\n",
        "        \"\"\"\n",
        "        param_grid = {\n",
        "            'classifier__max_depth': [3, 5, 7],\n",
        "            'classifier__min_child_weight': [1, 3, 5],\n",
        "            'classifier__gamma': [0.0, 0.1, 0.2],\n",
        "            'classifier__learning_rate': [0.01, 0.1],\n",
        "            'classifier__n_estimators': [200, 500],\n",
        "            'classifier__subsample': [0.8, 0.9],\n",
        "            'classifier__colsample_bytree': [0.8, 0.9]\n",
        "        }\n",
        "\n",
        "        # Validación cruzada estratificada\n",
        "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "        grid_search = GridSearchCV(\n",
        "            self.pipeline,\n",
        "            param_grid,\n",
        "            cv=cv,\n",
        "            scoring='accuracy',\n",
        "            n_jobs=-1,\n",
        "            verbose=2\n",
        "        )\n",
        "\n",
        "        grid_search.fit(X_train, y_train)\n",
        "\n",
        "        print(\"Mejores parámetros encontrados:\")\n",
        "        print(grid_search.best_params_)\n",
        "\n",
        "        self.pipeline = grid_search.best_estimator_\n",
        "        return grid_search.best_score_\n",
        "\n",
        "    def evaluate_model(self, X_test, y_test):\n",
        "        \"\"\"\n",
        "        Evalúa el modelo y genera visualizaciones detalladas\n",
        "        \"\"\"\n",
        "        y_pred = self.pipeline.predict(X_test)\n",
        "        y_prob = self.pipeline.predict_proba(X_test)\n",
        "\n",
        "        # Reporte de clasificación\n",
        "        print(\"\\nReporte de Clasificación:\")\n",
        "        print(classification_report(y_test, y_pred))\n",
        "\n",
        "        # Matriz de confusión\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        cm = confusion_matrix(y_test, y_pred)\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "        plt.title('Matriz de Confusión')\n",
        "        plt.ylabel('Valor Real')\n",
        "        plt.xlabel('Predicción')\n",
        "        plt.show()\n",
        "\n",
        "        # Importancia de características\n",
        "        feature_importance = pd.DataFrame(\n",
        "            self.pipeline.named_steps['classifier'].feature_importances_,\n",
        "            index=self.pipeline.named_steps['preprocessor'].get_feature_names_out(),\n",
        "            columns=['importance']\n",
        "        ).sort_values('importance', ascending=False)\n",
        "\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        feature_importance.head(10).plot(kind='bar')\n",
        "        plt.title('Top 10 Características más Importantes')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        return feature_importance\n",
        "\n",
        "def main():\n",
        "    # Cargar datos\n",
        "    train_df = pd.read_csv(\"train.csv\")\n",
        "    test_df = pd.read_csv(\"test.csv\")\n",
        "\n",
        "    # Crear instancia del pipeline\n",
        "    pipeline = SaberProXGBoostPipeline()\n",
        "\n",
        "    # Separar features y target\n",
        "    X_train = train_df.drop(['RENDIMIENTO_GLOBAL'], axis=1)\n",
        "    y_train = train_df['RENDIMIENTO_GLOBAL']\n",
        "\n",
        "    # Codificar target\n",
        "    y_train_encoded = pipeline.label_encoder.fit_transform(y_train)\n",
        "\n",
        "    # Crear y configurar pipeline\n",
        "    pipeline.create_preprocessing_pipeline(train_df)\n",
        "\n",
        "    # Preparar datos\n",
        "    X_train_processed = pipeline.prepare_data(X_train)\n",
        "\n",
        "    # Entrenar modelo\n",
        "    print(\"Entrenando modelo XGBoost...\")\n",
        "    best_score = pipeline.train_model(X_train_processed, y_train_encoded)\n",
        "    print(f\"\\nMejor score en validación cruzada: {best_score:.3f}\")\n",
        "\n",
        "    # Preparar y guardar predicciones\n",
        "    test_processed = pipeline.prepare_data(test_df)\n",
        "    predictions = pipeline.pipeline.predict(test_processed)\n",
        "    predictions = pipeline.label_encoder.inverse_transform(predictions)\n",
        "\n",
        "    submission = pd.DataFrame({\n",
        "        'ID': test_df['ID'],\n",
        "        'RENDIMIENTO_GLOBAL': predictions\n",
        "    })\n",
        "\n",
        "    submission.to_csv('submission_xgboost.csv', index=False)\n",
        "    print(\"\\nSubmission guardada como 'submission_xgboost.csv'\")\n",
        "\n",
        "    return pipeline, submission\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    pipeline, submission = main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtGpGW7OuRnX",
        "outputId": "bf40ccf0-3648-49ae-be22-11ae63c52ed4"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Entrenando modelo XGBoost...\n",
            "Fitting 5 folds for each of 432 candidates, totalling 2160 fits\n"
          ]
        }
      ]
    }
  ]
}