{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\\"
      ],
      "metadata": {
        "id": "HJuKQu3kMA6o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# solucion 2- usando random forest clasifier\n",
        "\n"
      ],
      "metadata": {
        "id": "yeNkMQqTdrmi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#descomprimir archivos\n",
        "!ls\n",
        "!unzip udea-ai4eng-20242.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anxRcTBaGAkM",
        "outputId": "5d111e8d-20bf-4b2c-af98-cd9671c3041f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data  udea-ai4eng-20242.zip\n",
            "Archive:  udea-ai4eng-20242.zip\n",
            "  inflating: submission_example.csv  \n",
            "  inflating: test.csv                \n",
            "  inflating: train.csv               \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "class SaberProPipelineEnhanced(SaberProPipeline):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.label_encoder = LabelEncoder()\n",
        "\n",
        "    def create_preprocessing_pipeline(self, df):\n",
        "        \"\"\"\n",
        "        Pipeline de preprocesamiento mejorado\n",
        "        \"\"\"\n",
        "        # Características derivadas\n",
        "        def create_derived_features(df):\n",
        "            df = df.copy()\n",
        "            # Ratio de autofinanciamiento y acceso a internet\n",
        "            df['RATIO_AUTOFINANCIAMIENTO'] = df['ESTU_PAGOMATRICULAPROPIO'] * df['ESTU_VALORMATRICULAUNIVERSIDAD']\n",
        "            # Indicador de ventaja socioeconómica\n",
        "            df['VENTAJA_SOCIOECONOMICA'] = (df['FAMI_ESTRATOVIVIENDA'] +\n",
        "                                          df['FAMI_TIENEINTERNET'] * 2) / 3\n",
        "            return df\n",
        "\n",
        "        # Características numéricas y categóricas\n",
        "        numeric_features = ['FAMI_ESTRATOVIVIENDA', 'ESTU_HORASSEMANATRABAJA',\n",
        "                          'ESTU_VALORMATRICULAUNIVERSIDAD', 'FAMI_TIENEINTERNET',\n",
        "                          'ESTU_PAGOMATRICULAPROPIO', 'RATIO_AUTOFINANCIAMIENTO',\n",
        "                          'VENTAJA_SOCIOECONOMICA']\n",
        "\n",
        "        categorical_features = ['ESTU_PRGM_DEPARTAMENTO']\n",
        "\n",
        "        # Transformadores con manejo de outliers\n",
        "        numeric_transformer = Pipeline(steps=[\n",
        "            ('scaler', RobustScaler())  # Más robusto contra outliers\n",
        "        ])\n",
        "\n",
        "        categorical_transformer = Pipeline(steps=[\n",
        "            ('onehot', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'))\n",
        "        ])\n",
        "\n",
        "        # Preprocessor\n",
        "        preprocessor = ColumnTransformer(\n",
        "            transformers=[\n",
        "                ('num', numeric_transformer, numeric_features),\n",
        "                ('cat', categorical_transformer, categorical_features)\n",
        "            ])\n",
        "\n",
        "        # Ensemble de modelos\n",
        "        estimators = [\n",
        "            ('rf', RandomForestClassifier(n_estimators=200, random_state=42)),\n",
        "            ('xgb', XGBClassifier(random_state=42)),\n",
        "            ('gb', GradientBoostingClassifier(random_state=42))\n",
        "        ]\n",
        "\n",
        "        voting_classifier = VotingClassifier(\n",
        "            estimators=estimators,\n",
        "            voting='soft'\n",
        "        )\n",
        "\n",
        "        # Pipeline completo\n",
        "        self.pipeline = Pipeline([\n",
        "            ('feature_engineering', FunctionTransformer(create_derived_features)),\n",
        "            ('preprocessor', preprocessor),\n",
        "            ('classifier', voting_classifier)\n",
        "        ])\n",
        "\n",
        "        return self.pipeline\n",
        "\n",
        "    def train_model(self, X_train, y_train):\n",
        "        \"\"\"\n",
        "        Entrenamiento mejorado con validación estratificada\n",
        "        \"\"\"\n",
        "        param_grid = {\n",
        "            'classifier__rf__n_estimators': [200, 300],\n",
        "            'classifier__rf__max_depth': [None, 15, 20],\n",
        "            'classifier__xgb__n_estimators': [200, 300],\n",
        "            'classifier__xgb__max_depth': [6, 8],\n",
        "            'classifier__gb__n_estimators': [200, 300],\n",
        "            'classifier__gb__max_depth': [6, 8]\n",
        "        }\n",
        "\n",
        "        # Validación cruzada estratificada\n",
        "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "        grid_search = GridSearchCV(\n",
        "            self.pipeline,\n",
        "            param_grid,\n",
        "            cv=cv,\n",
        "            scoring=['accuracy', 'f1_weighted', 'precision_weighted', 'recall_weighted'],\n",
        "            refit='f1_weighted',  # Optimizar para F1 ponderado\n",
        "            n_jobs=-1\n",
        "        )\n",
        "\n",
        "        grid_search.fit(X_train, y_train)\n",
        "\n",
        "        print(\"Mejores parámetros encontrados:\")\n",
        "        print(grid_search.best_params_)\n",
        "\n",
        "        self.pipeline = grid_search.best_estimator_\n",
        "        return grid_search.best_score_"
      ],
      "metadata": {
        "id": "FVbmPgoYdumm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import json\n",
        "import os\n",
        "\n",
        "class SaberProSubmissionEnhanced:  # Quitamos la herencia por ahora\n",
        "    def __init__(self):\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        self.submission_history = []\n",
        "        self.model_metrics = {}\n",
        "        self.pipeline = None\n",
        "\n",
        "    def fit_transform_target(self, y):\n",
        "        \"\"\"\n",
        "        Codifica las etiquetas del target y guarda el encoder\n",
        "        \"\"\"\n",
        "        return self.label_encoder.fit_transform(y)\n",
        "\n",
        "    def create_preprocessing_pipeline(self, df):\n",
        "        \"\"\"\n",
        "        Crea el pipeline de preprocesamiento\n",
        "        \"\"\"\n",
        "        # Eliminamos columnas no necesarias\n",
        "        columns_to_drop = ['ID', 'PERIODO', 'FAMI_EDUCACIONPADRE',\n",
        "                          'FAMI_EDUCACIONMADRE', 'ESTU_PRGM_ACADEMICO']\n",
        "\n",
        "        # Mapeos para variables categóricas\n",
        "        self.estrato_map = {\n",
        "            \"Estrato 1\": 1, \"Estrato 2\": 2, \"Estrato 3\": 3,\n",
        "            \"Estrato 4\": 4, \"Estrato 5\": 5, \"Estrato 6\": 6\n",
        "        }\n",
        "\n",
        "        self.horas_trabajo_map = {\n",
        "            '0': 0, 'Menos de 10 horas': 5,\n",
        "            'Entre 11 y 20 horas': 15, 'Entre 21 y 30 horas': 25,\n",
        "            'Más de 30 horas': 30\n",
        "        }\n",
        "\n",
        "        self.valor_matricula_map = {\n",
        "            'Entre 1 millón y menos de 2.5 millones': 1.75,\n",
        "            'Entre 2.5 millones y menos de 4 millones': 3.25,\n",
        "            'Menos de 500 mil': 0.25,\n",
        "            'Entre 500 mil y menos de 1 millón': 0.75,\n",
        "            'Entre 4 millones y menos de 5.5 millones': 4.75,\n",
        "            'Más de 7 millones': 7.75,\n",
        "            'Entre 5.5 millones y menos de 7 millones': 6.25,\n",
        "            'No pagó matrícula': 0\n",
        "        }\n",
        "\n",
        "        # Identificar columnas numéricas y categóricas\n",
        "        numeric_features = ['FAMI_ESTRATOVIVIENDA', 'ESTU_HORASSEMANATRABAJA',\n",
        "                          'ESTU_VALORMATRICULAUNIVERSIDAD', 'FAMI_TIENEINTERNET',\n",
        "                          'ESTU_PAGOMATRICULAPROPIO']\n",
        "\n",
        "        categorical_features = ['ESTU_PRGM_DEPARTAMENTO']\n",
        "\n",
        "        # Crear transformadores\n",
        "        numeric_transformer = Pipeline(steps=[\n",
        "            ('scaler', StandardScaler())\n",
        "        ])\n",
        "\n",
        "        categorical_transformer = Pipeline(steps=[\n",
        "            ('onehot', OneHotEncoder(drop='first', sparse_output=False))\n",
        "        ])\n",
        "\n",
        "        # Combinar transformadores\n",
        "        preprocessor = ColumnTransformer(\n",
        "            transformers=[\n",
        "                ('num', numeric_transformer, numeric_features),\n",
        "                ('cat', categorical_transformer, categorical_features)\n",
        "            ])\n",
        "\n",
        "        # Crear pipeline completo\n",
        "        self.pipeline = Pipeline([\n",
        "            ('preprocessor', preprocessor),\n",
        "            ('classifier', RandomForestClassifier(n_estimators=200, random_state=42))\n",
        "        ])\n",
        "\n",
        "        return self.pipeline\n",
        "\n",
        "    def prepare_data(self, df):\n",
        "        \"\"\"\n",
        "        Prepara los datos aplicando las transformaciones necesarias\n",
        "        \"\"\"\n",
        "        # Copiar el DataFrame\n",
        "        df_processed = df.copy()\n",
        "\n",
        "        # Eliminar columnas no necesarias\n",
        "        columns_to_drop = ['ID', 'PERIODO', 'FAMI_EDUCACIONPADRE',\n",
        "                          'FAMI_EDUCACIONMADRE', 'ESTU_PRGM_ACADEMICO']\n",
        "        df_processed = df_processed.drop(columns_to_drop, axis=1, errors='ignore')\n",
        "\n",
        "        # Aplicar mapeos\n",
        "        df_processed['FAMI_ESTRATOVIVIENDA'] = df_processed['FAMI_ESTRATOVIVIENDA'].fillna('Estrato 3').map(self.estrato_map)\n",
        "        df_processed['ESTU_HORASSEMANATRABAJA'] = df_processed['ESTU_HORASSEMANATRABAJA'].fillna('Más de 30 horas').map(self.horas_trabajo_map)\n",
        "        df_processed['ESTU_VALORMATRICULAUNIVERSIDAD'] = df_processed['ESTU_VALORMATRICULAUNIVERSIDAD'].fillna('Entre 1 millón y menos de 2.5 millones').map(self.valor_matricula_map)\n",
        "        df_processed['FAMI_TIENEINTERNET'] = df_processed['FAMI_TIENEINTERNET'].fillna('Si').map({'Si': 1, 'No': 0})\n",
        "        df_processed['ESTU_PAGOMATRICULAPROPIO'] = df_processed['ESTU_PAGOMATRICULAPROPIO'].fillna('No').map({'Si': 1, 'No': 0})\n",
        "\n",
        "        return df_processed\n",
        "\n",
        "    def train_model(self, X_train, y_train):\n",
        "        \"\"\"\n",
        "        Entrena el modelo usando GridSearchCV\n",
        "        \"\"\"\n",
        "        param_grid = {\n",
        "          'classifier__n_estimators': [100],  # Reduced to one value\n",
        "          'classifier__max_depth': [None, 10], # Reduced to two values\n",
        "          'classifier__min_samples_split': [2], # Reduced to one value\n",
        "          'classifier__min_samples_leaf': [1]   # Reduced to one value\n",
        "}\n",
        "\n",
        "        grid_search = GridSearchCV(\n",
        "            self.pipeline,\n",
        "            param_grid,\n",
        "            cv=5,\n",
        "            scoring='accuracy',\n",
        "            n_jobs=-1,\n",
        "            verbose=2\n",
        "        )\n",
        "\n",
        "        grid_search.fit(X_train, y_train)\n",
        "\n",
        "        print(\"Mejores parámetros encontrados:\")\n",
        "        print(grid_search.best_params_)\n",
        "\n",
        "        self.pipeline = grid_search.best_estimator_\n",
        "        return grid_search.best_score_\n",
        "\n",
        "    def prepare_submission(self, test_df, train_df):\n",
        "        \"\"\"\n",
        "        Prepara el archivo de submission con validación y análisis\n",
        "        \"\"\"\n",
        "        # Validación de datos\n",
        "        self._validate_data(test_df)\n",
        "\n",
        "        # Preparar datos de test\n",
        "        test_processed = self.prepare_data(test_df)\n",
        "\n",
        "        # Obtener predicciones\n",
        "        predictions = self.pipeline.predict(test_processed)\n",
        "\n",
        "        # Decodificar predicciones si es necesario\n",
        "        if hasattr(self, 'label_encoder') and self.label_encoder.classes_.size > 0:\n",
        "            predictions = self.label_encoder.inverse_transform(predictions)\n",
        "\n",
        "        # Crear DataFrame de submission\n",
        "        submission = pd.DataFrame({\n",
        "            'ID': test_df['ID'],\n",
        "            'RENDIMIENTO_GLOBAL': predictions\n",
        "        })\n",
        "\n",
        "        return submission\n",
        "\n",
        "    def _validate_data(self, test_df):\n",
        "        \"\"\"\n",
        "        Valida la integridad de los datos de test\n",
        "        \"\"\"\n",
        "        # Verificar columnas requeridas\n",
        "        required_columns = ['ID', 'FAMI_ESTRATOVIVIENDA', 'ESTU_HORASSEMANATRABAJA',\n",
        "                          'ESTU_VALORMATRICULAUNIVERSIDAD', 'FAMI_TIENEINTERNET',\n",
        "                          'ESTU_PAGOMATRICULAPROPIO', 'ESTU_PRGM_DEPARTAMENTO']\n",
        "\n",
        "        missing_columns = [col for col in required_columns if col not in test_df.columns]\n",
        "        if missing_columns:\n",
        "            raise ValueError(f\"Columnas faltantes en test_df: {missing_columns}\")\n",
        "\n",
        "    def save_submission(self, submission_df, filename='submission.csv'):\n",
        "        \"\"\"\n",
        "        Guarda el archivo de submission\n",
        "        \"\"\"\n",
        "        # Crear directorio para submissions si no existe\n",
        "        submission_dir = 'submissions'\n",
        "        os.makedirs(submission_dir, exist_ok=True)\n",
        "\n",
        "        # Agregar timestamp al nombre del archivo\n",
        "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "        filename_with_timestamp = f'submission_{timestamp}.csv'\n",
        "        filepath = os.path.join(submission_dir, filename_with_timestamp)\n",
        "\n",
        "        # Guardar submission\n",
        "        submission_df.to_csv(filepath, index=False)\n",
        "\n",
        "        print(f\"\\nSubmission guardada en: {filepath}\")\n",
        "        print(\"\\nPrimeras filas del archivo de submission:\")\n",
        "        print(submission_df.head())\n",
        "\n",
        "        print(\"\\nDistribución de predicciones:\")\n",
        "        print(submission_df['RENDIMIENTO_GLOBAL'].value_counts())\n",
        "\n",
        "def main():\n",
        "    # Cargar datos\n",
        "    train_df = pd.read_csv(\"train.csv\")\n",
        "    test_df = pd.read_csv(\"test.csv\")\n",
        "\n",
        "    # Separar features y target\n",
        "    X_train = train_df.drop(['RENDIMIENTO_GLOBAL'], axis=1)\n",
        "    y_train = train_df['RENDIMIENTO_GLOBAL']\n",
        "\n",
        "    # Crear instancia del pipeline\n",
        "    pipeline = SaberProSubmissionEnhanced()\n",
        "\n",
        "    # Codificar variable objetivo\n",
        "    y_train_encoded = pipeline.fit_transform_target(y_train)\n",
        "\n",
        "    # Crear y configurar el pipeline\n",
        "    pipeline.create_preprocessing_pipeline(train_df)\n",
        "\n",
        "    # Preparar datos de entrenamiento\n",
        "    X_train_processed = pipeline.prepare_data(X_train)\n",
        "\n",
        "    # Entrenar modelo\n",
        "    print(\"Entrenando modelo...\")\n",
        "    best_score = pipeline.train_model(X_train_processed, y_train_encoded)\n",
        "    print(f\"\\nMejor score en validación cruzada: {best_score:.3f}\")\n",
        "\n",
        "    # Generar predicciones\n",
        "    print(\"\\nGenerando predicciones para el conjunto de test...\")\n",
        "    submission = pipeline.prepare_submission(test_df, train_df)\n",
        "\n",
        "    # Guardar submission\n",
        "    pipeline.save_submission(submission)\n",
        "\n",
        "    return pipeline, submission\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    pipeline, submission = main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYcqPhVZfdgu",
        "outputId": "3e8f776a-c230-4eb6-bbc9-f62affbda6a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entrenando modelo...\n",
            "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
            "Mejores parámetros encontrados:\n",
            "{'classifier__max_depth': None, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__n_estimators': 100}\n",
            "\n",
            "Mejor score en validación cruzada: 0.388\n",
            "\n",
            "Generando predicciones para el conjunto de test...\n",
            "\n",
            "Submission guardada en: submissions/submission_20241118_041926.csv\n",
            "\n",
            "Primeras filas del archivo de submission:\n",
            "       ID RENDIMIENTO_GLOBAL\n",
            "0  550236               bajo\n",
            "1   98545         medio-alto\n",
            "2  499179               alto\n",
            "3  782980               bajo\n",
            "4  785185               bajo\n",
            "\n",
            "Distribución de predicciones:\n",
            "RENDIMIENTO_GLOBAL\n",
            "bajo          88702\n",
            "alto          84848\n",
            "medio-bajo    64203\n",
            "medio-alto    59033\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    }
  ]
}