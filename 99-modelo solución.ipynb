{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M100j-3N9kFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entrega Final Usando LigtGBM"
      ],
      "metadata": {
        "id": "e6-cz5aqqqJ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\"
      ],
      "metadata": {
        "id": "HJuKQu3kMA6o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wSCtkc6bq39z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#descomprimir archivos\n",
        "!ls\n",
        "!unzip udea-ai4eng-20242.zip\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anxRcTBaGAkM",
        "outputId": "d3b33e52-27f4-4adc-95d9-5fdb77c14475"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data  udea-ai4eng-20242.zip\n",
            "Archive:  udea-ai4eng-20242.zip\n",
            "  inflating: submission_example.csv  \n",
            "  inflating: test.csv                \n",
            "  inflating: train.csv               \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install lightgbm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPkWInitbi11",
        "outputId": "affe0b0b-41ff-4697-c110-48fe90426480"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.10/dist-packages (4.5.0)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from lightgbm) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from lightgbm) (1.13.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entrega final modleo 1"
      ],
      "metadata": {
        "id": "N_MK7EKLRQvn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# Tiene formato de código\n",
        "```\n",
        "\n",
        "# solucion 2\n",
        "con 60% de datos\n"
      ],
      "metadata": {
        "id": "yeNkMQqTdrmi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "import lightgbm as lgb\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "class SaberProLightGBMFast:\n",
        "    def __init__(self, sample_size=0.9):  # Por defecto usa 90% de los datos\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        self.sample_size = sample_size\n",
        "        self.pipeline = None\n",
        "\n",
        "    def fit_transform_target(self, y):\n",
        "        return self.label_encoder.fit_transform(y)\n",
        "\n",
        "    def prepare_data(self, df):\n",
        "        \"\"\"\n",
        "        Prepara los datos con mapeos simplificados\n",
        "        \"\"\"\n",
        "        df_processed = df.copy()\n",
        "\n",
        "        # Mapeos simplificados\n",
        "        df_processed['FAMI_ESTRATOVIVIENDA'] = df_processed['FAMI_ESTRATOVIVIENDA'].fillna('Estrato 3').map({\n",
        "            f\"Estrato {i}\": i for i in range(1, 7)\n",
        "        })\n",
        "\n",
        "        df_processed['ESTU_HORASSEMANATRABAJA'] = df_processed['ESTU_HORASSEMANATRABAJA'].fillna('0').map({\n",
        "            '0': 0, 'Menos de 10 horas': 5, 'Entre 11 y 20 horas': 15,\n",
        "            'Entre 21 y 30 horas': 25, 'Más de 30 horas': 35\n",
        "        })\n",
        "\n",
        "        # Simplificamos el mapeo de valor matrícula\n",
        "        valor_matricula_map = {\n",
        "            'Menos de 500 mil': 0.5,\n",
        "            'Entre 500 mil y menos de 1 millón': 1,\n",
        "            'Entre 1 millón y menos de 2.5 millones': 2,\n",
        "            'Entre 2.5 millones y menos de 4 millones': 3,\n",
        "            'Entre 4 millones y menos de 5.5 millones': 4,\n",
        "            'Entre 5.5 millones y menos de 7 millones': 5,\n",
        "            'Más de 7 millones': 6,\n",
        "            'No pagó matrícula': 0\n",
        "        }\n",
        "        df_processed['ESTU_VALORMATRICULAUNIVERSIDAD'] = df_processed['ESTU_VALORMATRICULAUNIVERSIDAD'].fillna('Entre 1 millón y menos de 2.5 millones').map(valor_matricula_map)\n",
        "\n",
        "        # Variables binarias\n",
        "        df_processed['FAMI_TIENEINTERNET'] = df_processed['FAMI_TIENEINTERNET'].fillna('Si').map({'Si': 1, 'No': 0})\n",
        "        df_processed['ESTU_PAGOMATRICULAPROPIO'] = df_processed['ESTU_PAGOMATRICULAPROPIO'].fillna('No').map({'Si': 1, 'No': 0})\n",
        "\n",
        "        # Eliminar columnas no necesarias\n",
        "        columns_to_drop = ['ID', 'PERIODO', 'FAMI_EDUCACIONPADRE',\n",
        "                          'FAMI_EDUCACIONMADRE', 'ESTU_PRGM_ACADEMICO']\n",
        "        df_processed = df_processed.drop(columns_to_drop, axis=1, errors='ignore')\n",
        "\n",
        "        return df_processed\n",
        "\n",
        "    def create_model(self):\n",
        "        \"\"\"\n",
        "        Crea un pipeline simplificado con LightGBM\n",
        "        \"\"\"\n",
        "        numeric_features = ['FAMI_ESTRATOVIVIENDA', 'ESTU_HORASSEMANATRABAJA',\n",
        "                          'ESTU_VALORMATRICULAUNIVERSIDAD', 'FAMI_TIENEINTERNET',\n",
        "                          'ESTU_PAGOMATRICULAPROPIO']\n",
        "\n",
        "        categorical_features = ['ESTU_PRGM_DEPARTAMENTO']\n",
        "\n",
        "        # Preprocessor simplificado\n",
        "        preprocessor = ColumnTransformer(\n",
        "            transformers=[\n",
        "                ('num', StandardScaler(), numeric_features),\n",
        "                ('cat', OneHotEncoder(drop='first', sparse_output=False), categorical_features)\n",
        "            ])\n",
        "\n",
        "        # Configuración básica de LightGBM\n",
        "        lgb_params = {\n",
        "            'objective': 'multiclass',\n",
        "            'boosting_type': 'gbdt',\n",
        "            'n_estimators': 100,\n",
        "            'num_leaves': 31,\n",
        "            'max_depth': 7,\n",
        "            'learning_rate': 0.1,\n",
        "            'subsample': 0.8,\n",
        "            'colsample_bytree': 0.8,\n",
        "            'random_state': 42,\n",
        "            'n_jobs': -1\n",
        "        }\n",
        "\n",
        "        # Pipeline simplificado\n",
        "        self.pipeline = Pipeline([\n",
        "            ('preprocessor', preprocessor),\n",
        "            ('classifier', lgb.LGBMClassifier(**lgb_params))\n",
        "        ])\n",
        "\n",
        "        return self.pipeline\n",
        "\n",
        "    def train_and_evaluate(self, X_train, y_train):\n",
        "        \"\"\"\n",
        "        Entrena el modelo con validación cruzada simple\n",
        "        \"\"\"\n",
        "        print(\"Tomando una muestra del\", self.sample_size * 100, \"% de los datos...\")\n",
        "\n",
        "        # Tomar una muestra del dataset\n",
        "        X_sample, _, y_sample, _ = train_test_split(\n",
        "            X_train, y_train,\n",
        "            train_size=self.sample_size,\n",
        "            stratify=y_train,\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "        print(f\"Tamaño de la muestra de entrenamiento: {len(X_sample)} registros\")\n",
        "\n",
        "        # Crear y entrenar el modelo\n",
        "        self.create_model()\n",
        "\n",
        "        print(\"Entrenando modelo...\")\n",
        "        # Usar cross_val_score para una evaluación rápida\n",
        "        scores = cross_val_score(\n",
        "            self.pipeline, X_sample, y_sample,\n",
        "            cv=3, scoring='f1_weighted', n_jobs=-1\n",
        "        )\n",
        "\n",
        "        print(\"\\nResultados de la validación cruzada:\")\n",
        "        print(f\"F1-weighted promedio: {scores.mean():.3f} (+/- {scores.std() * 2:.3f})\")\n",
        "\n",
        "        # Entrenar el modelo final con todos los datos de la muestra\n",
        "        self.pipeline.fit(X_sample, y_sample)\n",
        "\n",
        "        return scores.mean()\n",
        "\n",
        "    def prepare_submission(self, test_df):\n",
        "        \"\"\"\n",
        "        Prepara el archivo de submission\n",
        "        \"\"\"\n",
        "        test_processed = self.prepare_data(test_df)\n",
        "        predictions = self.pipeline.predict(test_processed)\n",
        "\n",
        "        if hasattr(self, 'label_encoder') and self.label_encoder.classes_.size > 0:\n",
        "            predictions = self.label_encoder.inverse_transform(predictions)\n",
        "\n",
        "        submission = pd.DataFrame({\n",
        "            'ID': test_df['ID'],\n",
        "            'RENDIMIENTO_GLOBAL': predictions\n",
        "        })\n",
        "\n",
        "        return submission\n",
        "\n",
        "    def save_submission(self, submission_df):\n",
        "        \"\"\"\n",
        "        Guarda el archivo de submission\n",
        "        \"\"\"\n",
        "        submission_dir = 'submissions'\n",
        "        os.makedirs(submission_dir, exist_ok=True)\n",
        "\n",
        "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "        filepath = os.path.join(submission_dir, f'submission_lgbm_fast_{timestamp}.csv')\n",
        "\n",
        "        submission_df.to_csv(filepath, index=False)\n",
        "\n",
        "        print(f\"\\nSubmission guardada en: {filepath}\")\n",
        "        print(\"\\nDistribución de predicciones:\")\n",
        "        print(submission_df['RENDIMIENTO_GLOBAL'].value_counts())\n",
        "\n",
        "def main():\n",
        "    print(\"Cargando datos...\")\n",
        "    train_df = pd.read_csv(\"train.csv\")\n",
        "    test_df = pd.read_csv(\"test.csv\")\n",
        "\n",
        "    X_train = train_df.drop(['RENDIMIENTO_GLOBAL'], axis=1)\n",
        "    y_train = train_df['RENDIMIENTO_GLOBAL']\n",
        "\n",
        "    # Crear pipeline con 30% de los datos\n",
        "    pipeline = SaberProLightGBMFast(sample_size=0.3)\n",
        "\n",
        "    # Codificar target\n",
        "    y_train_encoded = pipeline.fit_transform_target(y_train)\n",
        "\n",
        "    # Preparar datos\n",
        "    print(\"Preparando datos...\")\n",
        "    X_train_processed = pipeline.prepare_data(X_train)\n",
        "\n",
        "    # Entrenar y evaluar\n",
        "    print(\"\\nIniciando entrenamiento...\")\n",
        "    score = pipeline.train_and_evaluate(X_train_processed, y_train_encoded)\n",
        "\n",
        "    # Generar predicciones\n",
        "    print(\"\\nGenerando predicciones...\")\n",
        "    submission = pipeline.prepare_submission(test_df)\n",
        "\n",
        "    # Guardar submission\n",
        "    pipeline.save_submission(submission)\n",
        "\n",
        "    return pipeline, submission\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    pipeline, submission = main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmgJA9q2VYyV",
        "outputId": "e7e1d553-749c-4fdf-dfe9-a17247518780"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cargando datos...\n",
            "Preparando datos...\n",
            "\n",
            "Iniciando entrenamiento...\n",
            "Tomando una muestra del 30.0 % de los datos...\n",
            "Tamaño de la muestra de entrenamiento: 207750 registros\n",
            "Entrenando modelo...\n",
            "\n",
            "Resultados de la validación cruzada:\n",
            "F1-weighted promedio: 0.384 (+/- 0.001)\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010771 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 83\n",
            "[LightGBM] [Info] Number of data points in the train set: 207750, number of used features: 32\n",
            "[LightGBM] [Info] Start training from score -1.371986\n",
            "[LightGBM] [Info] Start training from score -1.387094\n",
            "[LightGBM] [Info] Start training from score -1.395026\n",
            "[LightGBM] [Info] Start training from score -1.391226\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "\n",
            "Generando predicciones...\n",
            "\n",
            "Submission guardada en: submissions/submission_lgbm_fast_20241123_214954.csv\n",
            "\n",
            "Distribución de predicciones:\n",
            "RENDIMIENTO_GLOBAL\n",
            "bajo          88839\n",
            "alto          88255\n",
            "medio-bajo    66308\n",
            "medio-alto    53384\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    }
  ]
}